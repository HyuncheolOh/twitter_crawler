#node, edge, properties 
from __future__ import division, absolute_import, print_function
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import json, os, sys
import numpy as np
import echo_chamber_util as e_util
from time import time
from operator import itemgetter
from graph_tool import util
from graph_tool.all import *
from draw_tools.cdf_plot import CDFPlot
from draw_tools.ccdf_plot import CCDFPlot
from draw_tools.line_plot import LinePlot

#construct echo chamber network 
def find_echo_chamber_network():
    filename = 'Data/echo_chamber2.json'
    with open(filename, 'r') as f:
        echo_chamber = json.load(f)

    all_echo_chambers = {} 
    count = 0
    for key in echo_chamber:
        e = echo_chamber[key]

        if len(e) > 1:
            #set uses  - echo chamber  
    
            all_echo_chambers.update({key:echo_chamber[key]})

            count += 1 

            if count > 10:
                break


    g = Graph(directed=False)
    keys = all_echo_chambers.keys()
    vprop = g.new_vertex_property("string")
    eprop = g.new_edge_property("string")
    eweight = g.new_edge_property("int")
    d = {}
    for key in keys:
        v = g.add_vertex()
        vprop[v] = key
        d[key] = v


    for i in range(0, len(keys)):
        key1 = keys[i]
        #v1 = util.find_vertex(g, vprop, key1)[0]
        v1 = d[key1]
        #if i % 100 == 0:
        #    print(i)
        for j in range(i+1, len(keys)):
            key2 = keys[j]
            intersection = set(all_echo_chambers[key1]) & set(all_echo_chambers[key2])
            if len(intersection) > 0:
                #print(intersection)
                #edge between key1 node & key2 node
                v2 = d[key2]
                e1 = g.add_edge(v1, v2)
                eprop[e1] = '%s_%s'%(str(v1), str(v2)) 
                eweight[e1] = len(intersection)
    
    g.vertex_properties['vertex'] = vprop
    g.edge_properties['edge'] = eprop
    g.edge_properties['weight'] = eweight
   
    #for v in g.vertices():
    #    for e in v.out_edges():
    #        print(e)

    #out_hist = vertex_hist(g, "out")
    #print(out_hist)
    print(g.get_out_degrees(g.get_vertices()))
    print(np.count_nonzero(g.get_out_degrees(g.get_vertices())))
    #print(g.get_in_degrees(g.get_vertices()))

    cascade_centrality_analysis(g, vprop)
    rumor_centrality_analysis(g, vprop)
    print('done')
    #g.save('graph2.xml.gz')
    #graph_draw(g, output='graph.pdf')


def cascade_centrality_analysis(g, vprop):
    print('Compare cascade characteristics of echo chamber users')
    pr = pagerank(g)
    vp, ep = betweenness(g)
    c = closeness(g)

    p_rank = {}; b_rank = {}; c_rank = {}
    i = 0
    import math
    for p_v, b_v, c_v in zip(pr, vp, c):
        if not math.isnan(p_v):
            p_rank[i] = p_v 
        if not math.isnan(b_v):
            b_rank[i] = b_v
        if not math.isnan(c_v):
            c_rank[i] = c_v
        i += 1

    p_sort = sorted(p_rank.items(), key=itemgetter(1), reverse=True)
    b_sort = sorted(b_rank.items(), key=itemgetter(1), reverse=True)
    c_sort = sorted(c_rank.items(), key=itemgetter(1), reverse=True)

    print(c_sort)
    c_breadth, c_depth, c_unique_users = e_util.get_cascade_max_breadth()
    
    keys1 = [vprop[item[0]] for item in p_sort[:100]]
    keys2 = [vprop[item[0]] for item in b_sort[:100]]
    keys3 = [vprop[item[0]] for item in c_sort[:100]]
    keys4 = [vprop[item[0]] for item in p_sort]
    keys5 = [vprop[item[0]] for item in b_sort]
    keys6 = [vprop[item[0]] for item in c_sort]

    #get unique rumor set from keys
    a = []; b = []; c = []; d= []; e = []; f = []

    with open('Data/echo_chamber2.json', 'r') as f:
        echo_chamber = json.load(f)

    #get unique echo chamber users
    echo_chamber_users1 = get_unique_echo_chamber_users(echo_chamber, keys1) 
    echo_chamber_users2 = get_unique_echo_chamber_users(echo_chamber, keys2) 
    echo_chamber_users3 = get_unique_echo_chamber_users(echo_chamber, keys3) 
    echo_chamber_users4 = get_unique_echo_chamber_users(echo_chamber, keys4) 
    echo_chamber_users5 = get_unique_echo_chamber_users(echo_chamber, keys5) 
    echo_chamber_users6 = get_unique_echo_chamber_users(echo_chamber, keys6) 

    #max_cascade1 = {};  max_cascade2 = {}; max_cascade3 = {}; max_cascade4 = {}; max_cascade5 = {}; max_cascade6 = {}; 
    #max_breadth1 = {};  max_breadth2 = {}; max_breadth3 = {}; max_breadth4 = {}; max_breadth5 = {}; max_breadth6 = {}; 
    #max_depth1 = {};  max_depth2 = {}; max_depth3 = {}; max_depth4 = {}; max_depth5 = {}; max_depth6 = {}; 
    #max_users1 = {};  max_users2 = {}; max_users3 = {}; max_users4 = {}; max_users5 = {}; max_users6 = {};
    pagerank1 = {}; pagerank2 = {}; betweenness1 = {}; betweenness2 = {}; closeness1 = {}; closeness2 = {}
    for item in ['max_depth', 'max_breadth', 'cascade', 'unique_users']:
        pagerank1[item] = {}
        pagerank2[item] = {}
        betweenness1[item] = {}
        betweenness2[item] = {}
        closeness1[item] = {}
        closeness2[item] = {}

    #get cacade info 
    files = os.listdir('Retweet')
    for postid in files:

        with open('Retweet/%s'%postid, 'r') as f:
            tweets = json.load(f)

        users1 = echo_chamber_users1.get(postid, [])
        users2 = echo_chamber_users2.get(postid, [])
        users3 = echo_chamber_users3.get(postid, [])
        users4 = echo_chamber_users4.get(postid, [])
        users5 = echo_chamber_users5.get(postid, [])
        users6 = echo_chamber_users6.get(postid, [])
        for tweet in tweets.values():
            origin_tweet = tweet['origin_tweet']
            if tweet['user'] in users1:
                pagerank1['max_depth'][origin_tweet] = c_depth[origin_tweet]
                pagerank1['cascade'][origin_tweet] = tweet['cascade']
                pagerank1['max_breadth'][origin_tweet] = c_breadth[origin_tweet]
                pagerank1['unique_users'][origin_tweet] = c_unique_users[origin_tweet]

            if tweet['user'] in users2:
                betweenness1['max_depth'][origin_tweet] = c_depth[origin_tweet]
                betweenness1['cascade'][origin_tweet] = tweet['cascade']
                betweenness1['max_breadth'][origin_tweet] = c_breadth[origin_tweet]
                betweenness1['unique_users'][origin_tweet] = c_unique_users[origin_tweet]

            if tweet['user'] in users3:
                closeness1['max_depth'][origin_tweet] = c_depth[origin_tweet]
                closeness1['cascade'][origin_tweet] = tweet['cascade']
                closeness1['max_breadth'][origin_tweet] = c_breadth[origin_tweet]
                closeness1['unique_users'][origin_tweet] = c_unique_users[origin_tweet]

            if tweet['user'] in users4:
                pagerank2['max_depth'][origin_tweet] = c_depth[origin_tweet]
                pagerank2['cascade'][origin_tweet] = tweet['cascade']
                pagerank2['max_breadth'][origin_tweet] = c_breadth[origin_tweet]
                pagerank2['unique_users'][origin_tweet] = c_unique_users[origin_tweet]

            if tweet['user'] in users5:
                betweenness2['max_depth'][origin_tweet] = c_depth[origin_tweet]
                betweenness2['cascade'][origin_tweet] = tweet['cascade']
                betweenness2['max_breadth'][origin_tweet] = c_breadth[origin_tweet]
                betweenness2['unique_users'][origin_tweet] = c_unique_users[origin_tweet]

            if tweet['user'] in users6:
                closeness2['max_depth'][origin_tweet] = c_depth[origin_tweet]
                closeness2['cascade'][origin_tweet] = tweet['cascade']
                closeness2['max_breadth'][origin_tweet] = c_breadth[origin_tweet]
                closeness2['unique_users'][origin_tweet] = c_unique_users[origin_tweet]

    #compare cascade, depth, breadth, users by centrality metric
    draw_cdf_graph([pagerank1['cascade'].values(), betweenness1['cascade'].values(), closeness1['cascade'].values()], 'Max Cascade of a Rumor', ['PageRank', 'Betweenness', 'Closeness'], 'Centrality', 'centrality_user_cascade')
    draw_cdf_graph([pagerank1['max_depth'].values(), betweenness1['max_depth'].values(), closeness1['max_depth'].values()], 'Max Cascade of a Rumor', ['PageRank', 'Betweenness', 'Closeness'], 'Centrality', 'centrality_user_depth')
    draw_cdf_graph([pagerank1['max_breadth'].values(), betweenness1['max_breadth'].values(), closeness1['max_breadth'].values()], 'Max Cascade of a Rumor', ['PageRank', 'Betweenness', 'Closeness'], 'Centrality', 'centrality_user_breadth')
    draw_cdf_graph([pagerank1['unique_users'].values(), betweenness1['unique_users'].values(), closeness1['unique_users'].values()], 'Max Cascade of a Rumor', ['PageRank', 'Betweenness', 'Closeness'], 'Centrality', 'centrality_user_users')

    draw_cdf_graph([pagerank1['cascade'], pagerank2['cascade']], 'Cascade Size', ['High Rank', 'All'], 'PageRank', 'centrality_user_cascade_pagerank')
    draw_cdf_graph([betweenness1['cascade'], betweenness2['cascade']], 'Cascade Size', ['High Rank', 'All'], 'Betweenness', 'centrality_user_cascade_betweenness')
    draw_cdf_graph([closeness1['cascade'], closeness2['cascade']], 'Cascade Size', ['High Rank', 'All'], 'Closeness', 'centrality_user_cascade_closeness')
    draw_cdf_graph([pagerank1['breadth'], pagerankd2['breadth']], 'Max Breadth of a Cascade', ['High Rank', 'All'], 'PageRank', 'centrality_user_breadth_pagerank')
    draw_cdf_graph([betweenness1['breadth'], betweenness2['breadth']], 'Max Breadth of a Cascade', ['High Rank', 'All'], 'Betweenness', 'centrality_user_breadth_betweenness')
    draw_cdf_graph([closeness1['breadth'], closeness2['breadth']], 'Max Breadth of a Cascade', ['High Rank', 'All'], 'Closeness', 'centrality_user_breadth_closeness')
    draw_cdf_graph([pagerank1['depth'], pagerank2['depth']], 'Max Depth of a Cascade', ['High Rank', 'All'], 'PageRank', 'centrality_user_depth_pagerank')
    draw_cdf_graph([betweenness1['depth'], betweenness2['depth']], 'Max Depth of a Cascade', ['High Rank', 'All'], 'Betweenness', 'centrality_user_depth_betweenness')
    draw_cdf_graph([closeness1['depth'], closeness2['depth']], 'Max Depth of a Cascade', ['High Rank', 'All'], 'Closeness', 'centrality_user_depth_closeness')
    draw_cdf_graph([pagerank1['unique_users'], pagerank2['unique_users']], 'Unique_users of a Cascade', ['High Rank', 'All'], 'PageRank', 'centrality_user_unique_users_pagerank')
    draw_cdf_graph([betweenness1['unique_users'], betweenness2['unique_users']], 'Unique_users of a Cascade', ['High Rank', 'All'], 'Betweenness', 'centrality_user_unique_users_betweenness')
    draw_cdf_graph([closeness1['unique_users'], closeness2['unique_users']], 'Unique_users of a Cascade', ['High Rank', 'All'], 'Closeness', 'centrality_user_unique_users_closeness')

def get_unique_echo_chamber_users(echo_chamber, keys):
    echo_chamber_users = {}
    for key in keys:
        users = echo_chamber[key]
        postids = key.split('_')
        for postid in postids:
            echo_chamber_users[postid] = echo_chamber_users.get(postid, {})
            for user in users:
                echo_chamber_users[postid][user] = 1

    return echo_chamber_users


def rumor_centrality_analysis(g, vprop):
    print('Compare rumor characteristics by centrality')
    pr = pagerank(g)
    vp, ep = betweenness(g)
    c = closeness(g)

    p_rank = {}; b_rank = {}; c_rank = {}
    i = 0
    import math
    for p_v, b_v, c_v in zip(pr, vp, c):
        if not math.isnan(p_v):
            p_rank[i] = p_v 
        if not math.isnan(b_v):
            b_rank[i] = b_v
        if not math.isnan(c_v):
            c_rank[i] = c_v
        i += 1

    p_sort = sorted(p_rank.items(), key=itemgetter(1), reverse=True)
    b_sort = sorted(b_rank.items(), key=itemgetter(1), reverse=True)
    c_sort = sorted(c_rank.items(), key=itemgetter(1), reverse=True)
    #return p_sort[:100], b_sort[:100], c_sort[:100]
    
    raw_data = {'pagerank' :[vprop[item[0]] for item in p_sort[:100]], 
            'betweenness':[vprop[item[0]] for item in b_sort[:100]],
            'closeness':[vprop[item[0]] for item in c_sort[:100]]}


    #raw_data2 = {'pagerank' :[vprop[item[0]] for item in p_sort[len(p_sort)-100:]], 
    #        'betweenness':[vprop[item[0]] for item in b_sort[len(b_sort)-100:]],
    #        'closeness':[vprop[item[0]] for item in c_sort[len(c_sort)-100:]]}

    
    raw_data2 = {'pagerank' :[vprop[item[0]] for item in p_sort], 
            'betweenness':[vprop[item[0]] for item in b_sort],
            'closeness':[vprop[item[0]] for item in c_sort]}

    with open('Data/echo_chamber2.json', 'r') as f:
        echo_chamber = json.load(f)

    #user num of rank 
    p_user = [len(echo_chamber[item]) for item in raw_data['pagerank']]
    b_user = [len(echo_chamber[item]) for item in raw_data['betweenness']]
    c_user = [len(echo_chamber[item]) for item in raw_data['closeness']]

    raw_data.update({'p_usernum':p_user, 'b_usernum':b_user, 'c_usernum':c_user})

    #from pandas import DataFrame
    #data = DataFrame(raw_data)
    #data.to_csv('Data/centrality.csv', mode='w')
    #print('Result saved in Data/centrality.csv')

    max_breadth, max_depth, max_unique_users, max_cascade = e_util.get_rumor_max_properties()
    
    keys = raw_data['pagerank']
    keys2 = raw_data['betweenness']
    keys3 = raw_data['closeness']
    keys4 = raw_data2['pagerank']
    keys5 = raw_data2['betweenness']
    keys6 = raw_data2['closeness']

    print('length', len(keys), len(keys4))
    a = []; b = []; c = []; d= []; e = []; f = []
    #unique rumor set 
    for i in range(100):
        a.extend(keys[i].split('_'))
        b.extend(keys2[i].split('_'))
        c.extend(keys3[i].split('_'))
    for i in range(len(keys4)):
        d.extend(keys4[i].split('_'))
    for i in range(len(keys5)):
        e.extend(keys5[i].split('_'))
    for i in range(len(keys6)):
        f.extend(keys6[i].split('_'))

    keys = list(set(a))
    keys2 = list(set(b))
    keys3 = list(set(c))
   
    cascade = [max_cascade[key] for key in keys]
    cascade2 = [max_cascade[key] for key in keys2]
    cascade3 = [max_cascade[key] for key in keys3]
    breadth = [max_breadth[key] for key in keys]
    breadth2 = [max_breadth[key] for key in keys2]
    breadth3 = [max_breadth[key] for key in keys3]
    u_users = [max_unique_users[key] for key in keys]
    u_users2 = [max_unique_users[key] for key in keys2]
    u_users3 = [max_unique_users[key] for key in keys3]
    depth = [max_depth[key] for key in keys]
    depth2 = [max_depth[key] for key in keys2]
    depth3 = [max_depth[key] for key in keys3]

    keys4 = list(set(d))
    keys5 = list(set(e))
    keys6 = list(set(f))
    cascade4 = [max_cascade[key] for key in keys4]
    cascade5 = [max_cascade[key] for key in keys5]
    cascade6 = [max_cascade[key] for key in keys6]
    breadth4 = [max_breadth[key] for key in keys4]
    breadth5 = [max_breadth[key] for key in keys5]
    breadth6 = [max_breadth[key] for key in keys6]
    u_users4 = [max_unique_users[key] for key in keys4]
    u_users5 = [max_unique_users[key] for key in keys5]
    u_users6 = [max_unique_users[key] for key in keys6]
    depth4 = [max_depth[key] for key in keys4]
    depth5 = [max_depth[key] for key in keys5]
    depth6 = [max_depth[key] for key in keys6]

    print('unique rumors from top 100 echo chambers %s'%len(set(keys)))
    print('common rumor num : %s'%(len(set(keys) & set(keys4))))
    #compare centrality and cascade, depth, breadth 
    draw_cdf_graph([cascade, cascade2, cascade3], 'Max Cascade of a Rumor', ['PageRank', 'Betweenness', 'Closeness'], 'Centrality', 'centrality_cascade')
    draw_cdf_graph([breadth, breadth2, breadth3], 'Max Breadth of a Rumor', ['PageRank', 'Betweenness', 'Closeness'], 'Centrality', 'centrality_breadth')
    draw_cdf_graph([depth, depth2, depth3], 'Max Depth of a Rumor', ['PageRank', 'Betweenness', 'Closeness'], 'Centrality', 'centrality_depth')
    draw_cdf_graph([u_users, u_users2, u_users3], 'Number of Users of a Rumor', ['PageRank', 'Betweenness', 'Closeness'], 'Centrality', 'centrality_users')

    #compare with high rank and all other rumors  
    draw_cdf_graph([cascade, cascade4], 'Max Cascade of a Rumor', ['High Rank', 'All'], 'PageRank', 'centrality_cascade_pagerank')
    draw_cdf_graph([cascade2, cascade5], 'Max Cascade of a Rumor', ['High Rank', 'All'], 'Betweenness', 'centrality_cascade_betweenness')
    draw_cdf_graph([cascade3, cascade6], 'Max Cascade of a Rumor', ['High Rank', 'All'], 'Closeness', 'centrality_cascade_closeness')
    draw_cdf_graph([breadth, breadth4], 'Max Breadth of a Rumor', ['High Rank', 'All'], 'PageRank', 'centrality_breadth_pagerank')
    draw_cdf_graph([breadth2, breadth5], 'Max Breadth of a Rumor', ['High Rank', 'All'], 'Betweenness', 'centrality_breadth_betweenness')
    draw_cdf_graph([breadth3, breadth6], 'Max Breadth of a Rumor', ['High Rank', 'All'], 'Closeness', 'centrality_breadth_closeness')
    draw_cdf_graph([depth, depth4], 'Max Depth of a Rumor', ['High Rank', 'All'], 'PageRank', 'centrality_depth_pagerank')
    draw_cdf_graph([depth2, depth5], 'Max Depth of a Rumor', ['High Rank', 'All'], 'Betweenness', 'centrality_depth_betweenness')
    draw_cdf_graph([depth3, depth6], 'Max Depth of a Rumor', ['High Rank', 'All'], 'Closeness', 'centrality_depth_closeness')
    draw_cdf_graph([u_users, u_users4], 'Number of Users', ['High Rank', 'All'], 'PageRank', 'centrality_users_pagerank')
    draw_cdf_graph([u_users2, u_users5], 'Number of Users', ['High Rank', 'All'], 'Betweenness', 'centrality_users_betweenness')
    draw_cdf_graph([u_users3, u_users6], 'Number of Users', ['High Rank', 'All'], 'Closeness', 'centrality_users_closeness')

def draw_cdf_graph(datas, x_label, legends, legends_title, file_name, log_scale=False):
    cdf = CDFPlot()
    cdf.set_log(log_scale)
    cdf.set_label(x_label, 'CDF')
    for data in datas:
        cdf.set_data(data, '')
    cdf.set_legends(legends, legends_title)
    cdf.save_image('%s/%s.png'%(folder_name, file_name))

def analyze_echo_chamber_network():
    g = load_graph('graph.xml.gz')
    vprop = g.vertex_properties['vertex']
    eprop = g.edge_properties['edge']
    eweight = g.edge_properties['weight']

    # Let's plot its in-degree distribution
    #out_hist = vertex_hist(g, "out")

    #print('degree max : %s, min : %s'%(max(in_hist), min(in_hist)))

    
    cascade_centrality_analysis(g, vprop)
    rumor_centrality_analysis(g, vprop)

    v_count = 0
    e_count = 0 
    for v in g.vertices():
        v_count += 1

    degree = g.get_out_degrees(g.get_vertices())
    print('Vertex Count : %s'%v_count)
    print('Vertices which have edges : %s'%(np.count_nonzero(g.get_out_degrees(g.get_vertices()))))
    print('Edge Count : %s'%(sum(degree)))
    #CDF and CCDF of degree of vertex
    cdf = CDFPlot()
    cdf.set_label('Degree', 'CDF')
    cdf.set_log(True)
    cdf.set_data(degree, '')
    cdf.save_image("%s/degree_cdf"%(folder_name))
    
    cdf = CCDFPlot()
    cdf.set_label('Degree', 'CCDF')
    cdf.set_log(True)
    cdf.set_data(degree)
    cdf.save_image("%s/degree_ccdf"%(folder_name))

if __name__ == "__main__":
    folder_name = 'Image/20181007'
    start = time()
    find_echo_chamber_network()
    #analyze_echo_chamber_network()
    end = time()
    print('%s takes'%(end - start))
